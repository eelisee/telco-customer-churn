{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline Overview\n",
    "\n",
    "This preprocessing pipeline outlines the steps necessary to prepare the Telco Customer Churn dataset for our modeling. Each step is designed to address specific aspects of data quality, transformation, and feature creation. We will cover each step in a separate jupyter notebook file.\n",
    "\n",
    "**Step 1: Data Loading**: Loading the datasets into the workspace, ensuring all necessary files are correctly imported for analysis. This includes the Kaggle dataset and the IBM datasets.\n",
    "\n",
    "**Step 2: Dataset Integration**: Combining relevant datasets into a single, unified dataset that will serve as the foundation for subsequent analysis.\n",
    "\n",
    "**Step 3: Handling Missing Values**: Identifying and addressing missing values in the dataset to ensure data integrity. This step ensures no significant gaps hinder the analysis.\n",
    "\n",
    "**Step 4: Data Type Conversion**: Converting data columns to appropriate data types to optimize memory usage and prepare for feature engineering. Ensure consistency across all columns.\n",
    "\n",
    "**Step 5: Data Exploration**: Perform initial exploratory data analysis (EDA) to understand the dataset's structure and characteristics, visualizing key features to gain insights into the data.\n",
    "\n",
    "**Step 6: Feature Engineering**: Creating new features from the existing data to enhance model performance and capture additional insights. This includes transformations and derived features.\n",
    "\n",
    "**Step 7: Outlier Detection**: Identifying and addressing outliers in the dataset to ensure they do not negatively impact the analysis or models.\n",
    "\n",
    "**Step 8: Dataset Splitting**: Splitting the dataset into training and testing subsets to prepare for model development and evaluation. This step ensures reproducibility and robust performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to Split the Dataset\n",
    "\n",
    "When working with a dataset of 7034 entries, it is crucial to split the data into training and testing subsets to evaluate the models performance accurately. Here are some common methods to split the dataset:\n",
    "\n",
    "1. **Train-Test Split**:\n",
    "    - This is the most straightforward method where the dataset is divided into two parts: training and testing sets.\n",
    "    - Typically, 70-80% of the data is used for training, and the remaining 20-30% is used for testing.\n",
    "\n",
    "2. **Stratified Shuffle Split**:\n",
    "    - This method ensures that the training and testing sets have the same proportion of class labels as the original dataset.\n",
    "    - It is particularly useful for imbalanced datasets.\n",
    "\n",
    "3. **K-Fold Cross-Validation**:\n",
    "    - This method splits the dataset into `k` equal-sized folds. The model is trained on `k-1` folds and tested on the remaining fold.\n",
    "    - This process is repeated `k` times, with each fold used exactly once as the test set.\n",
    "\n",
    "4. **Stratified K-Fold Cross-Validation**:\n",
    "    - Similar to K-Fold Cross-Validation but ensures that each fold has the same proportion of class labels.\n",
    "\n",
    "Each of these methods has its advantages and can be chosen based on the specific requirements of the analysis and the characteristics of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=5, random_state=7, test_size=0.2,\n",
      "            train_size=None)\n",
      "train: [4486 1855 3910 ... 4687  690 5658] test: [ 724  945 4710 ... 3730 1633 2496]\n",
      "train: [ 937 6833 3567 ... 4257 4716 5657] test: [6171  505 5903 ... 6911 2257 5571]\n",
      "train: [4793 2570 3406 ...  805 2103 1162] test: [  31 2128 2079 ... 2313 6356 5365]\n",
      "train: [6799  180 3061 ... 3155 2428  793] test: [3083 4441  980 ... 4552 2903 1479]\n",
      "train: [ 603 4034 6970 ... 1356 4927 1489] test: [5931 4868 2690 ... 3344 6206 5245]\n",
      "X shape: (7043, 115)\n",
      "X_train shape: (5634, 115)\n",
      "X_test shape: (1409, 115)\n",
      "y shape: (7043,)\n",
      "y_train shape: (5634,)\n",
      "y_test shape: (1409,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../2_data/telcocustomerchurn_featured.csv\")\n",
    "# Define the features and target variable\n",
    "X = df.drop(columns=['Customer ID', 'Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "\n",
    "# StratifiedShuffleSplit \n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=7)\n",
    "print(sss)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"train:\", train_index, \"test:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Save the train and test splits to CSV files\n",
    "X_train.to_csv(\"../2_data/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"../2_data/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../2_data/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../2_data/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
