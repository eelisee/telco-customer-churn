{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4689cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23b2ff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33128/1740308715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m                   \u001b[1;31m# Naive Bayes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m              \u001b[1;31m# Decision Tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder               # conversion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier          # Random Forest\n",
    "from sklearn.svm import SVC, LinearSVC                       # SVC\n",
    "from sklearn.linear_model import LogisticRegression          # Logic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier           # KNN\n",
    "from sklearn.naive_bayes import GaussianNB                   # Naive Bayes\n",
    "from sklearn.tree import DecisionTreeClassifier              # Decision Tree\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier     \n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "telcom=pd.read_csv(r\"F:\\data\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "telcom.head(10)\n",
    "telcom.shape\n",
    "telcom.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869d729",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Clean\n",
    "\n",
    "# Missing Value\n",
    "pd.isnull(telcom).sum()\n",
    "telcom[\"Churn\"].value_counts()\n",
    "telcom.info()\n",
    "\n",
    "telcom['TotalCharges']=telcom['TotalCharges'].convert_objects(convert_numeric=True) # convert_numeric=True表示强制转换数字(包括字符串)，不可转换的值变为NaN\n",
    "telcom[\"TotalCharges\"].dtypes\n",
    "\n",
    "# Check Missing Value Again\n",
    "pd.isnull(telcom[\"TotalCharges\"]).sum()\n",
    "# Delet them\n",
    "telcom.dropna(inplace=True)\n",
    "telcom.shape\n",
    "\n",
    "\n",
    "# Norms\n",
    "# Yes=1, No=0\n",
    "telcom['Churn'].replace(to_replace = 'Yes', value = 1,inplace = True)\n",
    "telcom['Churn'].replace(to_replace = 'No', value = 0,inplace = True)\n",
    "telcom['Churn'].head()\n",
    "\n",
    "telcom['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "telcom['Churn'].replace(to_replace='No',  value=0, inplace=True)\n",
    "telcom['Churn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0630de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "# Check churn customer proportion\n",
    "\"\"\"\n",
    "Parameters for pie chart:\n",
    "labels        - Explanation text displayed outside each slice\n",
    "explode       - Distance from the center for each slice\n",
    "startangle    - Starting angle for drawing; default is from x-axis positive direction counterclockwise. Setting to 90 starts from y-axis positive.\n",
    "shadow        - Whether to add shadow\n",
    "labeldistance - Label position relative to radius, if <1, displays inside pie chart\n",
    "autopct       - Controls the format of percentage labels, e.g., '%1.1f' indicates one decimal place\n",
    "pctdistance   - Position scale for autopct\n",
    "radius        - Controls pie chart radius\n",
    "\"\"\"\n",
    "churnvalue = telcom[\"Churn\"].value_counts()\n",
    "labels = telcom[\"Churn\"].value_counts().index\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 6,6\n",
    "plt.pie(churnvalue, labels=labels, colors=[\"whitesmoke\",\"yellow\"], explode=(0.1,0), autopct='%1.1f%%', shadow=True)\n",
    "plt.title(\"Proportions of Customer Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Effects of gender, senior citizen, partner, dependents on churn rate\n",
    "f, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "gender = sns.countplot(x=\"gender\", hue=\"Churn\", data=telcom, palette=\"Pastel2\") # palette parameter sets color, here as theme color Pastel2\n",
    "plt.xlabel(\"gender\")\n",
    "plt.title(\"Churn by Gender\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "seniorcitizen = sns.countplot(x=\"SeniorCitizen\", hue=\"Churn\", data=telcom, palette=\"Pastel2\")\n",
    "plt.xlabel(\"senior citizen\")\n",
    "plt.title(\"Churn by Senior Citizen\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "partner = sns.countplot(x=\"Partner\", hue=\"Churn\", data=telcom, palette=\"Pastel2\")\n",
    "plt.xlabel(\"partner\")\n",
    "plt.title(\"Churn by Partner\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "dependents = sns.countplot(x=\"Dependents\", hue=\"Churn\", data=telcom, palette=\"Pastel2\")\n",
    "plt.xlabel(\"dependents\")\n",
    "plt.title(\"Churn by Dependents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature extraction\n",
    "charges = telcom.iloc[:,1:20]\n",
    "# Encode features\n",
    "\"\"\"\n",
    "Encoding for discrete features:\n",
    "1. If values have no size meaning (e.g., color: [red, blue]), use one-hot encoding\n",
    "2. If values have size meaning (e.g., size: [X, XL, XXL]), use mapping to numbers {X:1, XL:2, XXL:3}\n",
    "\"\"\"\n",
    "corrDf = charges.apply(lambda x: pd.factorize(x)[0])\n",
    "corrDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct correlation matrix\n",
    "corr = corrDf.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation matrix with heatmap\n",
    "'''\n",
    "heatmap        - Displays coefficient matrix with heatmap\n",
    "linewidths     - Gap size between cells\n",
    "annot          - Whether to display coefficient value in each cell\n",
    "'''\n",
    "plt.figure(figsize=(20,16))\n",
    "ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n",
    "                 linewidths=0.2, cmap=\"YlGnBu\", annot=True)\n",
    "plt.title(\"Correlation between variables\")\n",
    "# Conclusion: From the above, there is a strong correlation between internet services, network security services, online backup services, device protection services, technical support services, streaming TV, and streaming movies. Multi-line business and phone service are also strongly correlated, and all show strong positive correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d68103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "tel_dummies = pd.get_dummies(telcom.iloc[:,1:21])\n",
    "tel_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9de7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between telecom churn and various variables\n",
    "plt.figure(figsize=(15,8))\n",
    "tel_dummies.corr()['Churn'].sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title(\"Correlations between Churn and variables\")\n",
    "\n",
    "\n",
    "# From the chart, it can be seen that variables 'gender' and 'PhoneService' are close to zero in correlation, so they have minimal effect on predicting telecom churn and can be discarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of services on churn rate\n",
    "covariables = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16,10))\n",
    "for i, item in enumerate(covariables):\n",
    "    plt.subplot(2,3,(i+1))\n",
    "    ax = sns.countplot(x=item, hue=\"Churn\", data=telcom, palette=\"Pastel2\", order=[\"Yes\",\"No\",\"No internet service\"])\n",
    "    plt.xlabel(str(item))\n",
    "    plt.title(\"Churn by \" + str(item))\n",
    "    i = i + 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of contract type on churn rate\n",
    "sns.barplot(x=\"Contract\", y=\"Churn\", data=telcom, palette=\"Pastel1\", order=['Month-to-month', 'One year', 'Two year'])\n",
    "plt.title(\"Churn by Contract type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of payment method on churn rate\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"PaymentMethod\", y=\"Churn\", data=telcom, palette=\"Pastel1\", order=['Bank transfer (automatic)', 'Credit card (automatic)', 'Electronic check','Mailed check'])\n",
    "plt.title(\"Churn by PaymentMethod type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. Data Preprocessing\n",
    "\n",
    "# As observed, 'CustomerID' is a random character string representing each customer and has no impact on modeling, so we choose to drop this column; 'gender' and 'PhoneService' have low correlation with churn rate, so we can ignore them.\n",
    "\n",
    "telcomvar = telcom.iloc[:,2:20]\n",
    "telcomvar.drop(\"PhoneService\", axis=1, inplace=True)\n",
    "\n",
    "# Extract ID\n",
    "telcom_id = telcom['customerID']\n",
    "\n",
    "telcomvar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eee5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize tenure, MonthlyCharges, and TotalCharges to mean of 0 and variance of 1\n",
    "\"\"\"\n",
    "Standardize the data to ensure each feature has a variance of 1 and mean of 0, so prediction results are not dominated by features with large values.\n",
    "\"\"\"\n",
    "scaler = StandardScaler(copy=False)\n",
    "# fit_transform() first fits the data, then transforms it into a standard format\n",
    "scaler.fit_transform(telcomvar[['tenure', 'MonthlyCharges', 'TotalCharges']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532099b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform() standardizes the data by centering and scaling\n",
    "telcomvar[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.transform(telcomvar[['tenure', 'MonthlyCharges', 'TotalCharges']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7719907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use boxplot to check for outliers in the data\n",
    "plt.figure(figsize=(8,4))\n",
    "numbox = sns.boxplot(data=telcomvar[['tenure', 'MonthlyCharges', 'TotalCharges']], palette=\"Set2\")\n",
    "plt.title(\"Check outliers of standardized tenure, MonthlyCharges, and TotalCharges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values in object type fields\n",
    "def uni(columnlabel):\n",
    "    print(columnlabel,\"--\" ,telcomvar[columnlabel].unique())  # The unique function removes duplicates and returns unique values\n",
    "\n",
    "telcomobject=telcomvar.select_dtypes(['object'])\n",
    "for i in range(0,len(telcomobject.columns)):\n",
    "    uni(telcomobject.columns[i])\n",
    "\n",
    "\n",
    "# Based on previous results, there is a \"No internet service\" in six variables. Customers who do not use any internet products have a low churn rate, so \"No internet service\" can be treated the same as \"No.\" Therefore, \"No internet service\" can be replaced with \"No.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "telcomvar.replace(to_replace='No internet service', value='No', inplace=True)\n",
    "telcomvar.replace(to_replace='No phone service', value='No', inplace=True)\n",
    "for i in range(0,len(telcomobject.columns)):\n",
    "    uni(telcomobject.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use Scikit-learn's label encoding to convert categorical data into integer encoding\n",
    "def labelencode(columnlabel):\n",
    "    telcomvar[columnlabel] = LabelEncoder().fit_transform(telcomvar[columnlabel])\n",
    "\n",
    "for i in range(0,len(telcomobject.columns)):\n",
    "    labelencode(telcomobject.columns[i])\n",
    "\n",
    "for i in range(0,len(telcomobject.columns)):\n",
    "    uni(telcomobject.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Model Building\n",
    "\n",
    "# ### (1) Create Training and Testing Datasets\n",
    "\"\"\"\n",
    "We need to split the dataset into training and testing sets for validation.\n",
    "Since our dataset is unbalanced, it’s best to use stratified cross-validation to ensure that both the training and testing sets retain the proportions of each class sample.\n",
    "StratifiedShuffleSplit is a cross-validation function that randomly splits the sample data into training and testing sets based on the given ratio.\n",
    "Parameter n_splits: the number of train/test sets to split into, can be set as needed, default is 10\n",
    "Parameters test_size and train_size are used to set the proportions for train/test\n",
    "Parameter random_state controls the random shuffling of samples\n",
    "\"\"\"\n",
    "X=telcomvar\n",
    "y=telcom[\"Churn\"].values\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "print(sss)\n",
    "print(\"Number of splits for training and testing data:\",sss.get_n_splits(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"train:\", train_index, \"test:\", test_index)\n",
    "    X_train,X_test=X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train,y_test=y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d70b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the size of the datasets\n",
    "print('Original data features:', X.shape,\n",
    "      'Training data features:',X_train.shape,\n",
    "      'Testing data features:',X_test.shape)\n",
    "\n",
    "print('Original data labels:', y.shape,\n",
    "      '   Training data labels:',y_train.shape,\n",
    "      '   Testing data labels:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### (2) Select Machine Learning Algorithms\n",
    "# Use classification algorithms, selecting 10 different classifiers here\n",
    "Classifiers=[[\"Random Forest\",RandomForestClassifier()],\n",
    "             [\"Support Vector Machine\",SVC()],\n",
    "             [\"LogisticRegression\",LogisticRegression()],\n",
    "             [\"KNN\",KNeighborsClassifier(n_neighbors=5)],\n",
    "             [\"Naive Bayes\",GaussianNB()],\n",
    "             [\"Decision Tree\",DecisionTreeClassifier()],\n",
    "             [\"AdaBoostClassifier\", AdaBoostClassifier()],\n",
    "             [\"GradientBoostingClassifier\", GradientBoostingClassifier()],\n",
    "             [\"XGB\", XGBClassifier()],\n",
    "             [\"CatBoost\", CatBoostClassifier(logging_level='Silent')]  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### (3) Train Models\n",
    "Classify_result=[]\n",
    "names=[]\n",
    "prediction=[]\n",
    "for name,classifier in Classifiers:\n",
    "    classifier=classifier\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred=classifier.predict(X_test)\n",
    "    recall=recall_score(y_test,y_pred)\n",
    "    precision=precision_score(y_test,y_pred)\n",
    "    class_eva=pd.DataFrame([recall,precision])\n",
    "    Classify_result.append(class_eva)\n",
    "    name=pd.Series(name)\n",
    "    names.append(name)\n",
    "    y_pred=pd.Series(y_pred)\n",
    "    prediction.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### (4) Evaluate Models\n",
    "\n",
    "# Evaluate models\n",
    "\"\"\"\n",
    "Recall: the proportion of true positives among all actual positives (higher values are better, with 1 being ideal)\n",
    "Precision: the proportion of true positives among all predicted positives (higher values are better, with 1 being ideal)\n",
    "F1-Score: a metric that combines Precision and Recall into a single score\n",
    "F1-Score ranges from 0 to 1, where 1 represents the best possible model and 0 represents the worst.\n",
    "\"\"\"\n",
    "\n",
    "names=pd.DataFrame(names)\n",
    "names=names[0].tolist()\n",
    "result=pd.concat(Classify_result,axis=1)\n",
    "result.columns=names\n",
    "result.index=[\"recall\",\"precision\",\"f1score\"]\n",
    "result\n",
    "\n",
    "\n",
    "# Conclusion: Among the 10 classification algorithms, Naive Bayes achieved the highest F1-Score of 63.31%, making it the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Implementation Plan\n",
    "\n",
    "# Predict the dataset features (as there is no provided prediction dataset, we select the last 10 rows as the dataset to predict)\n",
    "pred_X = telcomvar.tail(10)\n",
    "\n",
    "# Extract customerID\n",
    "pre_id = telcom_id.tail(10)\n",
    "\n",
    "# Use the Naive Bayes model to predict the churn status in the prediction dataset\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "pred_y = model.predict(pred_X)\n",
    "\n",
    "# Prediction results\n",
    "predDf = pd.DataFrame({'customerID':pre_id, 'Churn':pred_y})\n",
    "predDf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
